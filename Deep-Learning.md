# Deep Learning

For understanding Deep Learning, individuals need to understand the essence of data and how data works. Here, we need the data to work for us.

Deep Learning eliminates the job of Feature Extraction which was involved earlier with the


### Blogs

#### Optimizers

1. Adam

2. RMS Prop
From the TensorFlow perspective, explore the function [RMSProp Optimizer](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/RMSPropOptimizer)


#### Learning Algotithms

1. [Mini-Batch Gradient Descent](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)


#### Activation Functions


What are Activation Functions?
Activation Functions form one of the units of a Neuron. Layers comprises of Neurons.


1. ReLU
Rectified Linear Unit
    1. [A Practical Guide to ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7#targetText=ReLU%20stands%20for%20rectified%20linear,neural%20networks%2C%20especially%20in%20CNNs.)

#### CNNs

1. [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)



## Appendix

### Neuron
Neuron, inspired from Biology with all the electrical impulses
